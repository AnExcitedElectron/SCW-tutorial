---
title: "Running Jobs with Slurm"
author: "Colin Sauze"
date: "December 2017"
questions: 
 - "What is the difference between scratch and home?"
objectives: 
 - "Understand how to submit an interactive job using Slurm"
 - "Understand how to copy files between your computer and your SCW home/scratch directories"
keypoints:
 - "scratch and home are per site, no common storage between sites."
 - "scratch is faster and has no quotas, its not backed up. home is slower, smaller but backed up"
---


# Working with the scheduler

The scheduler is responsible for listening to your job requests, then finding the proper compute node that meets your job's resource requirements -- RAM, number of cores, time, etc --. It dispatches the job to a compute node, collects info about the completed work, and stores information about your job. If you've asked it to do so, it will even notify you about the status of your job (e.g. begin, end, fail, etc).


## Running interactive jobs

There are two ways to run jobs on a cluster. One, usually done at the start, is to get an
interactive/foreground session on a compute node. This will give you a command prompt on a compute node and let you run the commands of your choice there. 

If you want to experiment with some code and test it you should run it this way.

** Don't run jobs on the login nodes **


To get an interactive session, you issue the `srun` command with the appropriate parameters for requesting
the resources you require. For example:

~~~
srun --pty -p work -t 0-6:00 --mem 100 /bin/bash
~~~


This command requests from the scheduler a foreground/interactive job with the following resources:
```bash
--pty           # a parameter specific for the srun command for bash sessions
-p work     # the partition, or group of compute nodes to run on, note that only Bangor and Cardiff High Throughput (bwlxxx and cwlxxx) have a partition called work. 
-t 0-6:00       # time, in Days-Hours:Minutes format
--mem 100      # memory request, in MB
/bin/bash       # the program we want to run, which is the bash shell
```

Two additional, optional, parameters were left out; as such, SLURM will give us the defaults:
```bash
-n 1            # how many cores (CPUs) we want (default = 1)
-N 1            # how many nodes we want the cores on (not needed, as we're getting one core; required otherwise)
```

### Running batch jobs

The other method of running jobs on the cluster is by running a job in batch, using the `sbatch` command. On rare
occasion, you can use this just like the `srun` example, to run a simple command:

```
sbatch -t 0-0:1 --mem 10 --wrap="hostname"
```

```
Submitted batch job 3732341
```

This runs the hostname command which will tell us the name of the node it runs on. `--wrap` means that the command will be run inside a bash process, this is required for many programs to execute properly. After you submit the job sbatch will give you a job number, telling you the unique job number that Slurm has allocated. 

Once the job finishes (for this example this should be almost immediately) a file will be created with the job's output. This will be called `slurm-<jobnumber>.out`. The contents of this file will contain whatever the program would have usually displayed on the screen. 

```
cat slurm-3732341.out
```

```
cwc063
```

The other way is to create a batch submission script file, which has these parameters embedded inside, and submit your script to the scheduler. Create a file called ``batch.sh`` using the nano text editor and enter the following into it:

```
#!/bin/bash
hostname
```
The `#!/bin/bash` line tells the system that this is a bash program and that it needs to run it using bash, without this the job will not execute. 


Now submit the job 


```
sbatch -t 0-0:1 --mem 10 batch.sh
```
As before you should get a job number and an output file will be created. 


### Choosing the proper resources for your job
For both foreground and background submissions, you are requesting resources from the scheduler to run your job. These are:
* time
* memory
* # of cores (CPUs)
* # of nodes
* (sort of) queue or group of machines to use

Choosing resources is like playing a game with the scheduler: You want to request enough to get your job completed without failure, But request too much: your job is ‘bigger’ and thus harder to schedule. Request too little: if your job goes over that requested, it is killed. So you want to get it just right, and pad a little for wiggle room.

Another way to think of 'reserving' a compute node for you job is like making a reservation at a restaurant:
* if you bring more guests to your dinner, there won't be room at the restaurant, but the wait staff may try to fit them in. If so, things will be more crowded and the kitchen (and the whole restaurant) may slow down dramatically
* if you bring fewer guests and don't notify the staff in advance, the extra seats are wasted; no one else can take the empty places, and the restaurant may lose money.

“Never use a piece of bioinformatics software for the first time without looking to see what command-line options are available and what default parameters are being used”
	-- acgt.me · by Keith Bradnam
	
#### Time
This is determined by test runs that you do on your code during an interactive session.
Or, if you submit a batch job, over-ask first, check the amount of time actually needed,
then reduce time on later runs.

**Please!** Due to scheduler overhead, bundle commands for minimum of 10 minutes / job

#### Memory:
We recommend that you check the software docs for memory requirements. But often times these are not stated, so we can take another approach. On Odyssey, each job is allowed, on average, 4 GB RAM/core allocated. So, try 4 GB and do a trial run via `srun` or `sbatch`. If you're job was killed, look at your log files or immediately with squeue (see later). If it shows a memory error, you went over. Ask for more and try again.

Once the job has finished, ask the scheduler how much RAM was used by using the `sacct` command to get post-run job info:
```bash
sacct -j JOBID --format=JobID,JobName,ReqMem,MaxRSS,Elapsed	# RAM requested/used!!
```
The `ReqMem` field is how much you asked for and `MaxRSS` is how much was actually used. Now go back and adjust your RAM request in your sbatch command or submission script.


#### # of Cores
This is determined by your software, how anxious you are to get the work done, and how well your code scales. **NOTE! Throwing more cores at a job does not make it run faster!** This is a common mistake and will waste compute time and prevent other users from running jobs. Ensure your software can use multiple cores: Inspect the parameters for your software and look for options such as 'threads', 'processes', 'cpus'; this will often indicate that it has been parallelized. Then run test jobs to see how well it performs with multiple cores, inching slowing from 1 to 2, 4, 8, etc, assessing the decrease in time for the job run as you increase cores. Programs often do not scale well -- it's important to understand this so you can choose the appropriate number.

#### # of Nodes
For most software in biology, this choice is simple: 1. There are *very* few biology softare packages capable of running across multiple nodes. If they are capable, they will mention the use of technology called 'MPI' or 'openMPI'. Please talk to your local, friendly Research Computing Facilitator (e.g. ACI-REF or XSEDE Campus Champion) on how to make use of this feature.

#### Partitions (Queues)

Partitions, or queues, are a grouping of computers to run a certain profile of jobs. This could be maximum run time, # of cores used, amount of max RAM, etc. Partitions will vary significantly from site to site, so please check with your instructor for the appropriate ones to use, and why.

Here's a synopsis of partitions unique to Odyssey:

![Odyssey partitions](images/partitions-odyssey.jpg)
### Creating submission scripts

When creating submission scripts, use your favorite text editor and include the following sections:

**Required**
* shebang, or shell invocation
* Partition to submit to (comma separated)
* Any sofware or module loads
* actual commands to do work

**Recommended**
* job name
* # of cores
* # of nodes
* amount of time
* amount of memory
* STDOUT file
* STDERR file

**Optional**
* what mail notifications you want, if any
* email notification address 

All the scheduler directives need to be at the start of the file, then your module/software loads, and then your actual job commands. 

### Example batch script (SLURM)

The following is an example submission script for an example `fastqc` run, which is a single-core program:

```bash
#!/bin/bash
#
#SBATCH -p serial_requeue        # Partition to submit to (comma separated)
#SBATCH -J frog_fastqc           # Job name
#SBATCH -n 1                     # Number of cores
#SBATCH -N 1                     # Ensure that all cores are on one machine
#SBATCH -t 0-0:10                # Runtime in D-HH:MM (or use minutes)
#SBATCH --mem 1000               # Memory in MB
#SBATCH -o fastqc_%j.out         # File for STDOUT (with jobid = %j)
#SBATCH -e fastqc_%j.err         # File for STDERR (with jobid = %j)
#SBATCH --mail-type=ALL          # Type of email notification: BEGIN,END,FAIL,ALL
#SBATCH --mail-user=EMAIL@U.COM  # Email where notifications will be sent

source new-modules.sh; module load fastqc

cd my_output_directory

fastqc --casava -o fastqc_reports A1_R1.pair.fastq.gz

#... more processing goes here ...

```
Please refer to the following for other examples of SLURM scripts:
* [multicore job script](https://rc.fas.harvard.edu/wp-content/uploads/2015/05/Intro-to-Odyssey-v2-09.pdf#page=34)
* [multicore openMP job script](https://rc.fas.harvard.edu/wp-content/uploads/2015/05/Intro-to-Odyssey-v2-09.pdf#page=35)
* [multinode/MPI job script](https://rc.fas.harvard.edu/wp-content/uploads/2015/05/Intro-to-Odyssey-v2-09.pdf#page=36)

**Exercises**
* What would you need to change to make this a multicore (e.g. 8) job? Hint: more than one change is needed!
* Create a SLURM script from scratch, asking to run `bwa mem` on all your trimmed FASTQs on the general partition, with 2 GB of RAM, 1 core, for 1 hr.
* Create another SLURM script, now asking to run this same script using 4 cores and the parallel option for `bwa mem`.
* (Bonus!) If you had completed the script in the [Shell: Loops & Scripts](https://github.com/fasrc/DataC-shell-genomics/blob/gh-pages/lessons/03_loops_and_scripts.md) lesson, create a new script based on  your `generate_bad_reads_summary.sh` that contains the appropriate SLURM directives. When finished, submit this job to the cluster.

### Managing jobs and getting job information

There are several commands that you can use to control and get info about your jobs:

`scancel` will become your friend! At some point, you'll fire off one or more jobs, and realize you've
made a mistake. (What? You don't make them? Then you can forget about this command) Here are a 
few examples of `scancel` in action:

```bash
scancel JOBID										# specific job
scancel -u bfreeman								    # ALL my jobs
scancel -u bfreeman -J many_blast_jobs				# named jobs
scancel -u bfreeman -p bigmem						# ALL in partition
```
`squeue` will give you pending (to be done), running, and recently completed job info. Some examples:

```bash
squeue -u bfreeman									# jobs for bfreeman
squeue -u bfreeman --states=R | wc –l				# # of Running jobs
```

`sacct` will give you current and historical information, since time began or you were an HPC-infant,
whichever came first. More examples:

```bash
sacct -u bfreeman									# jobs for bfreeman
sacct -u bfreeman -p bigmem --starttime=9/1/14		# same+bigmem partition
sacct -j JOBID --format=JobID,JobName,ReqMem,MaxRSS,Elapsed # RAM requested & used!!
```

**Other info**
See our list of common SLURM commands at https://rc.fas.harvard.edu/resources/documentation/convenient-slurm-commands/

For those coming from another cluster/scheduler, check our our scheduler Rosetta stone: http://slurm.schedmd.com/rosetta.pdf

**Exercises**
At the start of these exercises, your instructor will start a number of jobs. Once started, please
complete the following questions:
* What `scancel` command would you issue to cancel pending jobs in the serial_requeue partition?
* What `squeue` command would you use to count how many jobs are in each state?
* What `sacct` command would you use to see all your jobs named BLAST run between Aug 1st and Aug 15th?
* (Bonus!) Give the command to ....


