---
title: "Running Jobs with Slurm"
author: "Colin Sauze"
date: "December 2017"
questions: 
 - "What is the difference between scratch and home?"
objectives: 
 - "Understand how to submit an interactive job using Slurm"
 - "Understand how to copy files between your computer and your SCW home/scratch directories"
keypoints:
 - "scratch and home are per site, no common storage between sites."
 - "scratch is faster and has no quotas, its not backed up. home is slower, smaller but backed up"
---


# Working with the scheduler

The scheduler is responsible for listening to your job requests, then finding the proper compute node that meets your job's resource requirements -- RAM, number of cores, time, etc --. It dispatches the job to a compute node, collects info about the completed work, and stores information about your job. If you've asked it to do so, it will even notify you about the status of your job (e.g. begin, end, fail, etc).


## Running interactive jobs

There are two ways to run jobs on a cluster. One, usually done at the start, is to get an
interactive/foreground session on a compute node. This will give you a command prompt on a compute node and let you run the commands of your choice there. 

If you want to experiment with some code and test it you should run it this way.

** Don't run jobs on the login nodes **


To get an interactive session, you first need to issue a `salloc` command to reserve some resources. 

~~~
salloc -n 1 --ntasks-per-node=1 
~~~
{: .bash}

The salloc command will respond now with a job ID number. 

~~~
salloc: Granted job allocation 3739432
~~~
{: output}

We have now allocated ourselves a host to run a program on. The `-n 1` tells slurm how many copies of the task we will be running. The `--ntasks-per-node=1` tells slurm that we will just be running one task for every node we are allocated. We could increase either of these numbers if we want to run multiple copies of a task and if we want to run more than one copy per node. 

To actually run a command we now need to issue the `srun` command. This also takes a `-n` parameter to tell Slurm how many copies of the job to run and it takes the name of the program to run. To run a job interactively we need another argument `--pty`. 

~~~
srun --pty -n 1 /bin/bash
~~~
{: .bash}


If you run command above you will see the hostname in the prompt change to the name of the compute node that slurm has allocated to you. In the example below the compute node is called `cwc001`. 
 
~~~
[jane.doe@cwl001 ~]$ srun -n 1 --pty /bin/bash
[jane.doe@cwc001 ~]$ 
~~~
{: output}

We are now logged into a compute node and can run any commands we wish and these will run on the compute node instead of the login node. You can also confirm the name of the host that you are connected to by running the `hostname` command.

~~~
hostname
~~~
{: .bash}

~~~
cwc001
~~~
{: output}


Once we are done working with the compute node we need to disconnect from it. The `exit` command will exit the bash program on the compute node causing us to disconnect. After this command is issued the hostname prompt should change back to the login node's name (e.g. cwl001). 

~~~
[jane.doe@cwc001 ~]$ exit
exit
[jane.doe@cwl001 ~]$ 
~~~
{: output}

At this point we still hold an allocation for a node and could run another job if we wish. We can confirm this by examining the queue of our jobs with the `squeue` command.

~~~
squeue
~~~
{: bash}

~~~
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           3739432      work     bash jane.doe  R       3:58      1 cwc001
~~~
{: output}

To relinquish the node allocation we need to issue another exit command. 

~~~
[jane.doe@cwl001 ~]$ exit
~~~
{: bash}

This will display a message that the job allocation is being relinquished and show us the same job ID number again.

~~~
exit
salloc: Relinquishing job allocation 3739432
~~~
{: output}

At this point our job is complete and we no longer hold any allocations. We can confirm this again with the `squeue` command.

~~~
[jane.doe@cwl001 ~]$ squeue
~~~
{: bash}

~~~
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
~~~
{: output}




This command requests from the scheduler a foreground/interactive job with the following resources:
```bash
--pty           # a parameter specific for the srun command for bash sessions
-p work     # the partition, or group of compute nodes to run on, note that only Bangor and Cardiff High Throughput (bwlxxx and cwlxxx) have a partition called work. 
-t 0-6:00       # time, in Days-Hours:Minutes format
--mem 100      # memory request, in MB
/bin/bash       # the program we want to run, which is the bash shell
```

Two additional, optional, parameters were left out; as such, SLURM will give us the defaults:
```bash
-n 1            # how many cores (CPUs) we want (default = 1)
-N 1            # how many nodes we want the cores on (not needed, as we're getting one core; required otherwise)
```

## Running batch jobs

For most situations we won't want to run a job interactively, instead we will want to submit it to the cluster have it do its work and return any output files to us. We might submit many copies of the same job with different parameters in this way. This method of working is known as batch processing. To do this we must first write some details about our job into a script file. Use `nano` (or your favourite command line text editor) and create a file containing the following:

~~~
nano batchjob.sh
~~~
{: bash}

~~~
#!/bin/bash --login
###
#job name
#SBATCH --job-name=hostname
#job stdout file
#SBATCH --output=hostname.out.%J
#job stderr file
#SBATCH --error=hostname.err.%J
#maximum job time in D-HH:MM
#SBATCH --time=0-00:01
#maximum memory of 10 megabytes
#SBATCH --mem-per-cpu=10
###

/bin/hostname

~~~
{: bash}

This is actually a bash script file containing all the commands that will be run. Lines beginning with a `#` are comments which bash will ignore. However lines that begin `#SBATCH` are instructions for the `sbatch` program. The first of these (`#SBATCH --job-name=hostname`) tells sbatch the name of the job, in this case we will call the job hostname. The `--output` line tells sbatch where output from the program should be sent, the `%J` in its name means the job number. The same applies for the `--error` line, except here it is for error messages that the program might generate, in most cases this file will be blank. The `--time` line limits how long the job can run for, this is specified in days, hours and minutes. The `--mem-per-cpu` tells Slurm how much memory to allow the job to use on each CPU it runs on, if the job exceeds this limit Slurm will automatically stop it. You can set this to zero for no limits. However by putting in a sensible number you can help allow other jobs to run on the same node. The final line specifies the actual commands which will be executed, in this case its the `hostname` command which will tell us the name of the compute node which ran our job.

Lets go ahead and submit this job with the `sbatch` command.

~~~
[jane.doe@cwl001 ~]$ sbatch batchjob.sh
~~~
{: bash}


sbatch will respond with the number of the job.

~~~
Submitted batch job 3739464
~~~
{: output}

Our job should only take a couple of seconds to run, but if we are fast we might see it in the `squeue` list.

~~~
[jane.doe@cwl001 ~]$ squeue
~~~
{: bash}

~~~
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           3739464      work hostname jane.doe  R       0:01      1 cwc001
~~~
{:output}

Once the job is completed two new files should be created, one called `hostname.out.3739464` and one called `hostname.err.3739464`. The `.out` file is the output from the command we ran and the `.err` is the errors from that command. Your job files will have different names as they will contain the job ID you were allocated and not `3739464`. Lets go ahead and look at the `.out` file:

~~~
[jane.doe@cwl001 ~]$ cat hostname.out.373464
~~~
{: bash}

~~~
cwc001
~~~
{: output}

If we check the `.err` file it should be blank:

~~~
[jane.doe@cwl001 ~]$ cat hostname.err.373464
~~~
{: bash}

~~~
~~~
{: output}


### Over-riding the sbatch options from the command line

As well as specifying options to sbatch in the batch file, they can specified on the command line too.
Lets edit our batch file to run the command `/bin/sleep 70` before `/bin/hostname`, this will cause it to wait for 70 seconds before exiting. As our job has a one minute limit this should fail and the hostname output will never happen.


~~~
[jane.doe@cwl001 ~]$ nano batchjob.sh
~~~
{: bash}


Edit the script to have the command `/bin/sleep 70` before the `hostname` command. 
~~~
#!/bin/bash --login
###
#job name
#SBATCH --job-name=hostname
#job stdout file
#SBATCH --output=hostname.out.%J
#job stderr file
#SBATCH --error=hostname.err.%J
#maximum job time in D-HH:MM
#SBATCH --time=0-00:01
#maximum memory of 10 megabytes
#SBATCH --mem-per-cpu=10
###

/bin/sleep 70
/bin/hostname
~~~
{: bash}


Now lets resubmit the job.

~~~
[jane.doe@cwl001 ~]$  sbatch batchjob.sh
~~~
{: bash}

~~~
Submitted batch job 3739465
~~~
{: output}

After approximately one minute the job will disappear from the `squeue` output, but this time the `.out` file should be empty and the `.err` file will contain an error message saying the job was cancelled:

~~~
[jane.doe@cwl001 ~]$  cat hostname.err.3739465
~~~
{: bash}


~~~
slurmstepd: error: *** JOB 3739465 ON cwc001 CANCELLED AT 2017-12-06T16:45:38 DUE TO TIME LIMIT ***
~~~
{: output}

~~~
[jane.doe@cwl001 ~]$ cat hostname.out.3739465
~~~
{: bash}


~~~
~~~
{: output}



Now lets override the time limit by giving the parameter `--time 0-0:2` to sbatch, this will set the time limit to two minutes and the job should complete.

~~~
[jane.doe@cwl001 ~]$ sbatch --time 0-0:2 batchjob.sh
~~~
{: bash}

~~~
 Submitted batch job 3739466
~~~
{: output}

After approximately 70 seconds the job will disappear from the squeue list and this we should have nothing in the `.err` file and a hostname in the `.out` file. 

~~~
[jane.doe@cwl001 ~]$ cat hostname.err.3739466
~~~
{: bash}

~~~
~~~
{: output}


~~~
[jane.doe@cwl001 ~]$ cat hostname.out.3739466
~~~
{: bash}

~~~
cwc001
~~~
{: output}


### Cancelling jobs

The `scancel` command can be used to cancel a job after its submitted. Lets go ahead and resubmit the job we just used.

~~~
[jane.doe@cwl001 ~]$  sbatch batchjob.sh
~~~
{: bash}

~~~
Submitted batch job 3739467
~~~
{: output}


Now (within 60 seconds) lets cancel the job.


~~~
[jane.doe@cwl001 ~]$  scancel 3739467
~~~
{: bash}

This will cancel the job, `squeue` will now show no record of it and there won't be a `.out` or `.err` file for it. 


### Listing jobs that have run

The `sacct` command lists all the jobs you have run. By default this shows the Job ID, the Job Name, partition, Account, number of CPUs used, the state of the job and how long it ran for.

~~~
[jane.doe@cwl001 ~]$  sacct
~~~
{: bash}


~~~
       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode 
------------ ---------- ---------- ---------- ---------- ---------- -------- 
3739467        hostname       work   hpcw0318          1    TIMEOUT      1:0 
3739467.bat+      batch              hpcw0318          1  COMPLETED      0:0 
3739468        hostname       work   hpcw0318          1    TIMEOUT      1:0 
3739468.bat+      batch              hpcw0318          1  CANCELLED     0:15 
3739472        hostname       work   hpcw0318          1  COMPLETED      0:0 
3739472.bat+      batch              hpcw0318          1  COMPLETED      0:0 
3739473        hostname       work   hpcw0318          1 CANCELLED+      0:0 
3739473.bat+      batch              hpcw0318          1  CANCELLED     0:15 
~~~
{: output}




### Choosing the proper resources for your job
For both foreground and background submissions, you are requesting resources from the scheduler to run your job. These are:
* time
* memory
* # of cores (CPUs)
* # of nodes
* (sort of) queue or group of machines to use

Choosing resources is like playing a game with the scheduler: You want to request enough to get your job completed without failure, But request too much: your job is ‘bigger’ and thus harder to schedule. Request too little: if your job goes over that requested, it is killed. So you want to get it just right, and pad a little for wiggle room.

Another way to think of 'reserving' a compute node for you job is like making a reservation at a restaurant:
* if you bring more guests to your dinner, there won't be room at the restaurant, but the wait staff may try to fit them in. If so, things will be more crowded and the kitchen (and the whole restaurant) may slow down dramatically
* if you bring fewer guests and don't notify the staff in advance, the extra seats are wasted; no one else can take the empty places, and the restaurant may lose money.

“Never use a piece of bioinformatics software for the first time without looking to see what command-line options are available and what default parameters are being used”
	-- acgt.me · by Keith Bradnam
	
#### Time
This is determined by test runs that you do on your code during an interactive session.
Or, if you submit a batch job, over-ask first, check the amount of time actually needed,
then reduce time on later runs.

**Please!** Due to scheduler overhead, bundle commands for minimum of 10 minutes / job

#### Memory:
We recommend that you check the software docs for memory requirements. But often times these are not stated, so we can take another approach. On Odyssey, each job is allowed, on average, 4 GB RAM/core allocated. So, try 4 GB and do a trial run via `srun` or `sbatch`. If you're job was killed, look at your log files or immediately with squeue (see later). If it shows a memory error, you went over. Ask for more and try again.

Once the job has finished, ask the scheduler how much RAM was used by using the `sacct` command to get post-run job info:
```bash
sacct -j JOBID --format=JobID,JobName,ReqMem,MaxRSS,Elapsed	# RAM requested/used!!
```
The `ReqMem` field is how much you asked for and `MaxRSS` is how much was actually used. Now go back and adjust your RAM request in your sbatch command or submission script.


#### # of Cores
This is determined by your software, how anxious you are to get the work done, and how well your code scales. **NOTE! Throwing more cores at a job does not make it run faster!** This is a common mistake and will waste compute time and prevent other users from running jobs. Ensure your software can use multiple cores: Inspect the parameters for your software and look for options such as 'threads', 'processes', 'cpus'; this will often indicate that it has been parallelized. Then run test jobs to see how well it performs with multiple cores, inching slowing from 1 to 2, 4, 8, etc, assessing the decrease in time for the job run as you increase cores. Programs often do not scale well -- it's important to understand this so you can choose the appropriate number.

#### # of Nodes
For most software in biology, this choice is simple: 1. There are *very* few biology softare packages capable of running across multiple nodes. If they are capable, they will mention the use of technology called 'MPI' or 'openMPI'. Please talk to your local, friendly Research Computing Facilitator (e.g. ACI-REF or XSEDE Campus Champion) on how to make use of this feature.

#### Partitions (Queues)

Partitions, or queues, are a grouping of computers to run a certain profile of jobs. This could be maximum run time, # of cores used, amount of max RAM, etc. Partitions will vary significantly from site to site, so please check with your instructor for the appropriate ones to use, and why.

Here's a synopsis of partitions unique to Odyssey:

![Odyssey partitions](images/partitions-odyssey.jpg)
### Creating submission scripts

When creating submission scripts, use your favorite text editor and include the following sections:

**Required**
* shebang, or shell invocation
* Partition to submit to (comma separated)
* Any sofware or module loads
* actual commands to do work

**Recommended**
* job name
* # of cores
* # of nodes
* amount of time
* amount of memory
* STDOUT file
* STDERR file

**Optional**
* what mail notifications you want, if any
* email notification address 

All the scheduler directives need to be at the start of the file, then your module/software loads, and then your actual job commands. 

### Example batch script (SLURM)

The following is an example submission script for an example `fastqc` run, which is a single-core program:

```bash
#!/bin/bash
#
#SBATCH -p serial_requeue        # Partition to submit to (comma separated)
#SBATCH -J frog_fastqc           # Job name
#SBATCH -n 1                     # Number of cores
#SBATCH -N 1                     # Ensure that all cores are on one machine
#SBATCH -t 0-0:10                # Runtime in D-HH:MM (or use minutes)
#SBATCH --mem 1000               # Memory in MB
#SBATCH -o fastqc_%j.out         # File for STDOUT (with jobid = %j)
#SBATCH -e fastqc_%j.err         # File for STDERR (with jobid = %j)
#SBATCH --mail-type=ALL          # Type of email notification: BEGIN,END,FAIL,ALL
#SBATCH --mail-user=EMAIL@U.COM  # Email where notifications will be sent

source new-modules.sh; module load fastqc

cd my_output_directory

fastqc --casava -o fastqc_reports A1_R1.pair.fastq.gz


### Email Alerts from Slurm

You can receive email alerts when your job begins and ends by adding the following to your Slurm script. Set `--mail-type` to `END` if you just want to be alerted about jobs completing. 

~~~
#SBATCH --mail-user=abc1@aber.ac.uk
#SBATCH --mail-type=ALL
~~~
{: bash}



#... more processing goes here ...

```
Please refer to the following for other examples of SLURM scripts:
* [multicore job script](https://rc.fas.harvard.edu/wp-content/uploads/2015/05/Intro-to-Odyssey-v2-09.pdf#page=34)
* [multicore openMP job script](https://rc.fas.harvard.edu/wp-content/uploads/2015/05/Intro-to-Odyssey-v2-09.pdf#page=35)
* [multinode/MPI job script](https://rc.fas.harvard.edu/wp-content/uploads/2015/05/Intro-to-Odyssey-v2-09.pdf#page=36)

**Exercises**
* What would you need to change to make this a multicore (e.g. 8) job? Hint: more than one change is needed!
* Create a SLURM script from scratch, asking to run `bwa mem` on all your trimmed FASTQs on the general partition, with 2 GB of RAM, 1 core, for 1 hr.
* Create another SLURM script, now asking to run this same script using 4 cores and the parallel option for `bwa mem`.
* (Bonus!) If you had completed the script in the [Shell: Loops & Scripts](https://github.com/fasrc/DataC-shell-genomics/blob/gh-pages/lessons/03_loops_and_scripts.md) lesson, create a new script based on  your `generate_bad_reads_summary.sh` that contains the appropriate SLURM directives. When finished, submit this job to the cluster.

### Managing jobs and getting job information

There are several commands that you can use to control and get info about your jobs:

`scancel` will become your friend! At some point, you'll fire off one or more jobs, and realize you've
made a mistake. (What? You don't make them? Then you can forget about this command) Here are a 
few examples of `scancel` in action:

```bash
scancel JOBID										# specific job
scancel -u bfreeman								    # ALL my jobs
scancel -u bfreeman -J many_blast_jobs				# named jobs
scancel -u bfreeman -p bigmem						# ALL in partition
```
`squeue` will give you pending (to be done), running, and recently completed job info. Some examples:

```bash
squeue -u bfreeman									# jobs for bfreeman
squeue -u bfreeman --states=R | wc –l				# # of Running jobs
```

`sacct` will give you current and historical information, since time began or you were an HPC-infant,
whichever came first. More examples:

```bash
sacct -u bfreeman									# jobs for bfreeman
sacct -u bfreeman -p bigmem --starttime=9/1/14		# same+bigmem partition
sacct -j JOBID --format=JobID,JobName,ReqMem,MaxRSS,Elapsed # RAM requested & used!!
```

**Other info**
See our list of common SLURM commands at https://rc.fas.harvard.edu/resources/documentation/convenient-slurm-commands/

For those coming from another cluster/scheduler, check our our scheduler Rosetta stone: http://slurm.schedmd.com/rosetta.pdf

**Exercises**
At the start of these exercises, your instructor will start a number of jobs. Once started, please
complete the following questions:
* What `scancel` command would you issue to cancel pending jobs in the serial_requeue partition?
* What `squeue` command would you use to count how many jobs are in each state?
* What `sacct` command would you use to see all your jobs named BLAST run between Aug 1st and Aug 15th?
* (Bonus!) Give the command to ....


