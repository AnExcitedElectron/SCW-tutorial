<h2 id="objectives">Objectives</h2>
<ul>
  <li>Why use a cluster?</li>
  <li>Understand how a cluster is organized</li>
  <li>Understand what a filesystem is and which are appropriate to use</li>
  <li>Know how to interact with the scheduler</li>
  <li>Be comfortable creating a batch script and submitting one</li>
  <li>Know how to get info about jobs and to control them</li>
</ul>

<h2 id="prior-knowledgepre-requesites">Prior Knowledge/Pre-requesites</h2>

<ul>
  <li>Basic use of the Linux command line, as covered in the Software Carpentry Introduction to the Unix Shell Lesson.</li>
  <li>An account on HPC Wales.</li>
</ul>

<h2 id="toc">TOC</h2>
<ul>
  <li><a href="#cluster-basics">Cluster basics</a>
    <ul>
      <li><a href="#what-are-some-of-reasons-to-access-a-remote-computer-system">What are some of reasons to access a remote computer system?</a></li>
      <li><a href="#advantages-of-using-hpchtc-vs-cloud-systems">Advantages of using HPC/HTC vs. Cloud systems</a></li>
      <li><a href="#what-does-a-cluster-look-like">What does a cluster look like?</a></li>
    </ul>
  </li>
  <li><a href="#filesystems">Filesystems</a></li>
  <li><a href="#using--installing-software">Using &amp; installing software</a></li>
  <li><a href="#working-with-the-scheduler">Working with the scheduler</a>
    <ul>
      <li><a href="#running--submitting-jobs">Running &amp; submitting jobs</a></li>
      <li><a href="#choosing-the-proper-resources-for-your-job">Choosing the proper resources for your job</a></li>
      <li><a href="#creating-submission-scripts">Creating submission scripts</a></li>
      <li><a href="#example-batch-script-SLURM">Example batch script (SLURM)</a></li>
      <li><a href="#managing-jobs-and-getting-job-information">Managing jobs and getting job information</a></li>
    </ul>
  </li>
  <li><a href="#best-Practices">Best Practices</a></li>
  <li><a href="#distributed-System-Definitions-and-stacks">Distributed System Definitions and stacks:</a></li>
  <li><a href="#hpc-vs-cloud">HPC vs. Cloud:</a></li>
  <li><a href="#resources">Resources:</a></li>
</ul>

<h2 id="cluster-basics">Cluster basics</h2>

<p>Clusters, otherwise know as high-performance computing (HPC) or high-throughput computing systems, belong to a class of computing environments known as Advanced CyberInfrastructure (ACI). ACI resources also include other high-end compute systems such as distributed databases, large-scale fileystems, and software-defined networks. These tools are becoming the <em>de facto</em> standard tools in most research disciplines today.</p>

<h3 id="what-are-some-of-reasons-to-use-a-cluster">What are some of reasons to use a cluster?</h3>

<ul>
  <li>Your computer does not have enough resources to run the desired analysis. <em>E.g.</em> memory, processors, disk space, or network bandwidth.</li>
  <li>You want to produce results faster than your computer can.</li>
  <li>You cannot install software in your computer. That is, the application does not have support for your operating system, conflicts with other existing applications, or softare licensing does not allow for installation on personal laptops.</li>
  <li>You want to leave something running while your computer would be turned off or doing something else.</li>
</ul>

<h3 id="what-does-a-cluster-look-like">What does a cluster look like?</h3>

<p>“High Performance Computing most generally refers to the practice of aggregating computing power in a way that delivers much higher performance than one could get out of a typical desktop computer or workstation in order to solve large problems in science, engineering, or business.” –http://insidehpc.com/hpc-basic-training/what-is-hpc/</p>

<p>Clusters are simply a grouping of computers with the same components (RAM, disk, processors/cores, and networking cards) as those in your desktop or laptop, but with more umph! and are networked with high-speed interconnect that can be accessed (indirectly) through software, the scheduler, that manages simultaneous execution of jobs, or analyses, by multiple persons.</p>

<p><img src="images/cluster-generic.png" alt="Overview of a compute cluster" /></p>

<p>The user accesses the compute cluster through one or more login nodes, and submits jobs to the scheduler, which will dispatch to and collect the completed work from the compute nodes. Frequently, clusters have shared disks, or filesystems, of various flavors where you can store your data, programs, and use for in-job execution (working or scratch areas)</p>

<h2 id="hpc-wales-and-super-computing-wales">HPC Wales and Super Computing Wales</h2>

<h3 id="hpc-wales">HPC Wales</h3>

<p>HPC Wales ran from 2010 to 2015 and provided clusters in Aberystwyth, Bangor, Cardiff, Glamorgan and Swansea. The Aberystwyth and Glamorgan systems have been decommissioned, but the Bangor, Cardiff and Swansea systems are still active.</p>

<h3 id="scw">SCW</h3>

<p>Super Computing Wales (SCW) is a new project to replace HPC Wales. It started in 2017 and runs for 5 years. It will include new systems in Cardiff and Swansea, but these haven’t been installed yet.</p>

<h3 id="how-to-get-access">How to get access?</h3>

<p>Email support@hpcwales.co.uk with completed project and account forms. 
Everyone on this course should have an account already.</p>

<h3 id="logging-in">Logging in</h3>

<p>Your username is usually <code class="highlighter-rouge">firstname.surname</code></p>

<p><code class="highlighter-rouge">ssh username@login.hpcwales.co.uk</code></p>

<p>Windows users should use PuTTY</p>

<h3 id="whats-available">What’s available?</h3>

<p>The <code class="highlighter-rouge">hpcwhosts</code> command</p>

<p>hpcwhosts
HPC Wales Clusters Available</p>

<h2 id="phase----system-location--type-------------login-nodes">Phase    System Location &amp; Type             Login Node(s)</h2>
<p>1        Cardiff High Throughput            cwl001   cwl002   cwl003
1        Bangor Medium Processing           bwl001   bwl002
2        Swansea Capability/Capacity/GPU    ssl001   ssl002   ssl003
2        Cardiff Capacity/GPU               csl001   csl002</p>

<p>Cardiff High Throughput ( login nodes cwl001,cwl002,cwl003)</p>

<div class="highlighter-rouge"><pre class="highlight"><code>162x Westmere 2x6 core, 36GB RAM
4x Nehalem dual CPU, 128GB RAM
1x Nehalem 8 CPU, 512GB RAM
</code></pre>
</div>

<p>Cardiff Capacity (login nodes csl001, csl002)</p>

<div class="highlighter-rouge"><pre class="highlight"><code>384x Sandy Bridge 2x8 core, 64GB RAM
</code></pre>
</div>

<p>Cardiff GPU (login nodes csl001, csl002)</p>

<div class="highlighter-rouge"><pre class="highlight"><code>16x Sandy Bridge 2x8 core, 64GB RAM, Nvidia Tesla M
</code></pre>
</div>

<p>Swansea (login nodes ssl001,ssl002,ssl003)</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Capability:

16x Sandy Bridge 2x8 core, 128GB RAM
240x Sandy Bridge 2x8 core, 64GB RAM

Capacity:

128x Sandy Bridge 2x8 core, 64GB RAM

GPU:

16x  Sandy Bridge 2x8 core, 64GB RAM, Nvidia Tesla M2090 512 core 6GB RAM
</code></pre>
</div>

<p>Bangor</p>

<div class="highlighter-rouge"><pre class="highlight"><code>650 Westmere cores
</code></pre>
</div>

<h3 id="slurm">Slurm</h3>

<p>Slurm is the management software used on HPC Wales. It lets you submit (and monitor or cancel) jobs to the cluster and chooses where to run them.</p>

<p>Other clusters might run different job management software such as LSF, Sun Grid Engine or Condor, although they all operate along similar principles.</p>

<h3 id="how-busy-is-the-cluster">How busy is the cluster?</h3>

<p>The sinfo command tells us the state of the cluster. It lets us know what nodes are available, how busy they are and what state they are in.</p>

<p>Clusters are sometimes divided up into partitions. This might separate some nodes which are different to the others (e.g. they have more memory, GPUs or different processors).</p>

<p>PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
work*        up   infinite      2 drain* bwc[022,052]
work*        up   infinite      1  down* bwc016
work*        up   infinite     13    mix bwc[001-002,010-012,031-036,050-051]
work*        up   infinite     38  alloc bwc[003-009,013-015,017-021,023-030,037-049,053-054]
long         up   infinite      2 drain* bwc[022,052]
long         up   infinite      1  down* bwc016
long         up   infinite     13    mix bwc[001-002,010-012,031-036,050-051]
long         up   infinite     38  alloc bwc[003-009,013-015,017-021,023-030,037-049,053-054]</p>

<p>work* means this is the default partition. 
AVAIL tells us if the partition is available.
TIMELIMIT tells us if there’s a time limit for jobs
NODES is the number of nodes in the this partition.
STATE, drain means the node will become unavailable once the current job ends. down is off, allow is allocated and mix is …</p>

<p><strong>Exercises</strong>
* Login to the login node. 
* Run hpcwhosts and pick a host to login to
* Login to that host
* run sinfo command and discuss with your neighbour what you think might be going on.
* Try using sinfo –long, does this give any more insights?</p>

<h2 id="filesystems-and-storage">Filesystems and Storage</h2>

<p>What is a filesystem?
Storage on most compute systems is not what and where you think they are! Physical disks are bundled together into a virtual volume; this virtual volume may represent one filesystem, or may be divided up, or partitioned, into multiple filesystems. And your directories then reside within one of these fileystems. Filesystems are accessed over the network through mount points.</p>

<p><img src="images/filesystems-generic.png" alt="Filesystem definition diagram" />
There are often multiple storage/filesystems options available for you to do your work. The most common are:
* home: where you land when you first login
* “working” directories, like SCRATCH or TEMP</p>

<p>Home folders are great for keeping information specific to your account, your workflows, and your environment. Typically, these are backed up and are limited in space. Note that these are usually appropriate for small amounts of work – up to 10 or so jobs – as these are on filesystems with large #s of other accounts on low-throughput/low bandwidth disk infrastructure.</p>

<p>“Working directories, often called TEMP, SCRATCH, or WORK, are often specialized, high-availability &amp; high-speed systems designed especially for large volumes of read and write operations found on HPC/HTC systems. Often times this is not backed and files are deleted after an aged period of time – on HPC Wales, this happens for files &gt; 60 days old. Your typical workflow pattern is to stage files in this location prior to or part of your job, do your work, and then copy your final data back to your home.</p>

<p>Here’s a synopsis of filesystems on HPC Wales:</p>

<p><img src="images/filesystems-odyssey.jpg" alt="Odyssey filesystems" /></p>

<p><strong>Important!! Ensure that you use the proper filesystem for your work, as improper usage can negatively affect other people’s jobs on those same physical disks.</strong></p>

<p><strong>Exercises</strong>
* What filesystem are you currently on? Can you figure that out?
* Use the <code class="highlighter-rouge">df .</code> command. Or <code class="highlighter-rouge">df -h .</code> command. What are you looking at? 
* Try moving to a different directory (e.g. <code class="highlighter-rouge">cd ~</code> or <code class="highlighter-rouge">cd /scratch</code>) and try this command again. What has changed?
* Try logging into a different cluster and compare the amount of disk space available.</p>

<h2 id="using--installing-software">Using &amp; installing software</h2>

<p>On most cluster systems, all the software installed is not immediately available for use;
instead, it is loaded into your environment incrementally using amodule system.</p>

<p>The module command controls this. 
You can get a list of available software with the <code class="highlighter-rouge">module avail</code> command. This should return a long list of available software.</p>

<p>Lets load Python version 3.5.1 as a module:</p>

<p>If we first try to run python3 it won’t work</p>

<p><code class="highlighter-rouge">$ python3
-bash: python3: command not found</code></p>

<p>But if we now load the module and try again it should:</p>

<p><code class="highlighter-rouge">$ module load python/3.5.1
$ python3
Python 3.5.1 (default, Jun 22 2016, 13:43:55) 
[GCC Intel(R) C++ gcc 4.4.7 mode] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; 
</code></p>

<h3 id="installing-python-modules">Installing Python modules</h3>

<p>Python is a popular language for researchers to use and has modules (aka libraries) that do all kinds of useful things, but sometimes the module you need won’t be installed. Many modules can be installed using the pip (or pip3 with python3) command. Before we can install modules we need to load the http-proxy module which allows the login node access to the internet via another computer (the proxy).</p>

<p><code class="highlighter-rouge">module load http-proxy
</code></p>

<p>We also don’t have permissions to install pip modules for the whole system, so we need to give pip the <code class="highlighter-rouge">--user</code> argument which tells us to install in our home directory. Note that this install will only be for the HPC Wales system you’re logged into, so if you install a module in Cardiff it won’t be available on the Swansea systems but will work on other Cardiff systems.</p>

<p><code class="highlighter-rouge">pip3 install --user &lt;modulename&gt;</code>`</p>

<p>Lets install the sklean module, this is a module which is useful for machine learning tasks. 
<code class="highlighter-rouge">pip3 install --user sklearn
Collecting sklearn
  Downloading sklearn-0.0.tar.gz
Collecting scikit-learn (from sklearn)
  Downloading scikit_learn-0.19.1-cp35-cp35m-manylinux1_x86_64.whl (12.2MB)
    100% |████████████████████████████████| 12.2MB 50kB/s 
Installing collected packages: scikit-learn, sklearn
  Running setup.py install for sklearn ... done
Successfully installed scikit-learn-0.19.1 sklearn-0.0
You are using pip version 8.1.2, however version 9.0.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
</code></p>

<p><em>For Perl &amp; Python modules or R packages</em>, we encourage you to set up directories in your
home and/or lab folders for installing your own copies locally.</p>

<p><em>If software you need is not installed</em>, we encourage you to do local installs in your home
or lab folder for bleeding-edge releases, software you are testing, or software used
only by your lab. For programs that are commonly used by your domain, field, 
or department, please submit a 
<a href="email support@hpcwales.co.uk">software install request</a>.
Note that due to demand and the complex nature of software installs, it may take a while for us to complete these requests.</p>

<h2 id="working-with-the-scheduler">Working with the scheduler</h2>

<p>As mentioned before, the scheduler is responsible for listening to your job requests, then finding the proper compute node that meets your job’s resource requirements – RAM, # cores, time, etc – dispatches the job to that compute node, collects info about the completed work, and stores information about your job. If you’ve asked it to do so, it will even notify you about the status of your job (e.g. begin, end, fail, etc).</p>

<h3 id="running-interactive-jobs">Running interactive jobs</h3>

<p>There are two ways to run jobs on a cluster. One, usually done at the start, is to get an
interactive/foreground session on a compute node. This looks and behaves exactly as when you first log into a compute
cluster, on a login node, but the work is being done a compute node, a worker node on the cluster. This is 
usually a best practice technique, and should be done for all work that will tie up resources (e.g. CPU-
or memory-intensive tasks).</p>

<p>To get an interactive session, you issue the <code class="highlighter-rouge">srun</code> command with the appropriate parameters for requesting
the resources you require. For example:</p>

<p><code class="highlighter-rouge">bash
srun --pty -p work -t 0-6:00 --mem 100 /bin/bash
</code></p>

<p>This command requests from the scheduler a foreground/interactive job with the following resources:
<code class="highlighter-rouge">bash
--pty           # a parameter specific for the srun command for bash sessions
-p work     # the partition, or group of compute nodes to run on, note that only Bangor and Cardiff High Throughput (bwlxxx and cwlxxx) have a partition called work. 
-t 0-6:00       # time, in Days-Hours:Minutes format
--mem 100      # memory request, in MB
/bin/bash       # the program we want to run, which is the bash shell
</code></p>

<p>Two additional, optional, parameters were left out; as such, SLURM will give us the defaults:
<code class="highlighter-rouge">bash
-n 1            # how many cores (CPUs) we want (default = 1)
-N 1            # how many nodes we want the cores on (not needed, as we're getting one core; required otherwise)
</code></p>

<h3 id="running-batch-jobs">Running batch jobs</h3>

<p>The other method of running jobs on the cluster is by running a job in batch, using the <code class="highlighter-rouge">sbatch</code> command. On rare
occasion, you can use this just like the <code class="highlighter-rouge">srun</code> example, to run a simple command:</p>

<p><code class="highlighter-rouge">
sbatch -t 0-0:1 --mem 10 --wrap="hostname"
</code></p>

<p><code class="highlighter-rouge">
Submitted batch job 3732341
</code></p>

<p>This runs the hostname command which will tell us the name of the node it runs on. <code class="highlighter-rouge">--wrap</code> means that the command will be run inside a bash process, this is required for many programs to execute properly. After you submit the job sbatch will give you a job number, telling you the unique job number that Slurm has allocated.</p>

<p>Once the job finishes (for this example this should be almost immediately) a file will be created with the job’s output. This will be called <code class="highlighter-rouge">slurm-&lt;jobnumber&gt;.out</code>. The contents of this file will contain whatever the program would have usually displayed on the screen.</p>

<p><code class="highlighter-rouge">
cat slurm-3732341.out
</code></p>

<p><code class="highlighter-rouge">
cwc063
</code></p>

<p>The other way is to create a batch submission script file, which has these parameters embedded inside, and submit your script to the scheduler. Create a file called <code class="highlighter-rouge">batch.sh</code> using the nano text editor and enter the following into it:</p>

<p><code class="highlighter-rouge">
<span class="c">#!/bin/bash</span>
hostname
</code>
The <code class="highlighter-rouge"><span class="c">#!/bin/bash</span></code> line tells the system that this is a bash program and that it needs to run it using bash, without this the job will not execute.</p>

<p>Now submit the job</p>

<p><code class="highlighter-rouge">
sbatch -t 0-0:1 --mem 10 batch.sh
</code>
As before you should get a job number and an output file will be created.</p>

<h3 id="choosing-the-proper-resources-for-your-job">Choosing the proper resources for your job</h3>
<p>For both foreground and background submissions, you are requesting resources from the scheduler to run your job. These are:
* time
* memory
* # of cores (CPUs)
* # of nodes
* (sort of) queue or group of machines to use</p>

<p>Choosing resources is like playing a game with the scheduler: You want to request enough to get your job completed without failure, But request too much: your job is ‘bigger’ and thus harder to schedule. Request too little: if your job goes over that requested, it is killed. So you want to get it just right, and pad a little for wiggle room.</p>

<p>Another way to think of ‘reserving’ a compute node for you job is like making a reservation at a restaurant:
* if you bring more guests to your dinner, there won’t be room at the restaurant, but the wait staff may try to fit them in. If so, things will be more crowded and the kitchen (and the whole restaurant) may slow down dramatically
* if you bring fewer guests and don’t notify the staff in advance, the extra seats are wasted; no one else can take the empty places, and the restaurant may lose money.</p>

<p>“Never use a piece of bioinformatics software for the first time without looking to see what command-line options are available and what default parameters are being used”
	– acgt.me · by Keith Bradnam</p>

<h4 id="time">Time</h4>
<p>This is determined by test runs that you do on your code during an interactive session.
Or, if you submit a batch job, over-ask first, check the amount of time actually needed,
then reduce time on later runs.</p>

<p><strong>Please!</strong> Due to scheduler overhead, bundle commands for minimum of 10 minutes / job</p>

<h4 id="memory">Memory:</h4>
<p>We recommend that you check the software docs for memory requirements. But often times these are not stated, so we can take another approach. On Odyssey, each job is allowed, on average, 4 GB RAM/core allocated. So, try 4 GB and do a trial run via <code class="highlighter-rouge">srun</code> or <code class="highlighter-rouge">sbatch</code>. If you’re job was killed, look at your log files or immediately with squeue (see later). If it shows a memory error, you went over. Ask for more and try again.</p>

<p>Once the job has finished, ask the scheduler how much RAM was used by using the <code class="highlighter-rouge">sacct</code> command to get post-run job info:
<code class="highlighter-rouge">bash
sacct -j JOBID --format=JobID,JobName,ReqMem,MaxRSS,Elapsed	# RAM requested/used!!
</code>
The <code class="highlighter-rouge">ReqMem</code> field is how much you asked for and <code class="highlighter-rouge">MaxRSS</code> is how much was actually used. Now go back and adjust your RAM request in your sbatch command or submission script.</p>

<h4 id="of-cores"># of Cores</h4>
<p>This is determined by your software, how anxious you are to get the work done, and how well your code scales. <strong>NOTE! Throwing more cores at a job does not make it run faster!</strong> This is a common mistake and will waste compute time and prevent other users from running jobs. Ensure your software can use multiple cores: Inspect the parameters for your software and look for options such as ‘threads’, ‘processes’, ‘cpus’; this will often indicate that it has been parallelized. Then run test jobs to see how well it performs with multiple cores, inching slowing from 1 to 2, 4, 8, etc, assessing the decrease in time for the job run as you increase cores. Programs often do not scale well – it’s important to understand this so you can choose the appropriate number.</p>

<h4 id="of-nodes"># of Nodes</h4>
<p>For most software in biology, this choice is simple: 1. There are <em>very</em> few biology softare packages capable of running across multiple nodes. If they are capable, they will mention the use of technology called ‘MPI’ or ‘openMPI’. Please talk to your local, friendly Research Computing Facilitator (e.g. ACI-REF or XSEDE Campus Champion) on how to make use of this feature.</p>

<h4 id="partitions-queues">Partitions (Queues)</h4>

<p>Partitions, or queues, are a grouping of computers to run a certain profile of jobs. This could be maximum run time, # of cores used, amount of max RAM, etc. Partitions will vary significantly from site to site, so please check with your instructor for the appropriate ones to use, and why.</p>

<p>Here’s a synopsis of partitions unique to Odyssey:</p>

<p><img src="images/partitions-odyssey.jpg" alt="Odyssey partitions" />
### Creating submission scripts</p>

<p>When creating submission scripts, use your favorite text editor and include the following sections:</p>

<p><strong>Required</strong>
* shebang, or shell invocation
* Partition to submit to (comma separated)
* Any sofware or module loads
* actual commands to do work</p>

<p><strong>Recommended</strong>
* job name
* # of cores
* # of nodes
* amount of time
* amount of memory
* STDOUT file
* STDERR file</p>

<p><strong>Optional</strong>
* what mail notifications you want, if any
* email notification address</p>

<p>All the scheduler directives need to be at the start of the file, then your module/software loads, and then your actual job commands.</p>

<h3 id="example-batch-script-slurm">Example batch script (SLURM)</h3>

<p>The following is an example submission script for an example <code class="highlighter-rouge">fastqc</code> run, which is a single-core program:</p>

<p>```bash
#!/bin/bash
#
#SBATCH -p serial_requeue        # Partition to submit to (comma separated)
#SBATCH -J frog_fastqc           # Job name
#SBATCH -n 1                     # Number of cores
#SBATCH -N 1                     # Ensure that all cores are on one machine
#SBATCH -t 0-0:10                # Runtime in D-HH:MM (or use minutes)
#SBATCH –mem 1000               # Memory in MB
#SBATCH -o fastqc_%j.out         # File for STDOUT (with jobid = %j)
#SBATCH -e fastqc_%j.err         # File for STDERR (with jobid = %j)
#SBATCH –mail-type=ALL          # Type of email notification: BEGIN,END,FAIL,ALL
#SBATCH –mail-user=EMAIL@U.COM  # Email where notifications will be sent</p>

<p>source new-modules.sh; module load fastqc</p>

<p>cd my_output_directory</p>

<p>fastqc –casava -o fastqc_reports A1_R1.pair.fastq.gz</p>

<h1 id="more-processing-goes-here-">… more processing goes here …</h1>

<p>```
Please refer to the following for other examples of SLURM scripts:
* <a href="https://rc.fas.harvard.edu/wp-content/uploads/2015/05/Intro-to-Odyssey-v2-09.pdf#page=34">multicore job script</a>
* <a href="https://rc.fas.harvard.edu/wp-content/uploads/2015/05/Intro-to-Odyssey-v2-09.pdf#page=35">multicore openMP job script</a>
* <a href="https://rc.fas.harvard.edu/wp-content/uploads/2015/05/Intro-to-Odyssey-v2-09.pdf#page=36">multinode/MPI job script</a></p>

<p><strong>Exercises</strong>
* What would you need to change to make this a multicore (e.g. 8) job? Hint: more than one change is needed!
* Create a SLURM script from scratch, asking to run <code class="highlighter-rouge">bwa mem</code> on all your trimmed FASTQs on the general partition, with 2 GB of RAM, 1 core, for 1 hr.
* Create another SLURM script, now asking to run this same script using 4 cores and the parallel option for <code class="highlighter-rouge">bwa mem</code>.
* (Bonus!) If you had completed the script in the <a href="https://github.com/fasrc/DataC-shell-genomics/blob/gh-pages/lessons/03_loops_and_scripts.md">Shell: Loops &amp; Scripts</a> lesson, create a new script based on  your <code class="highlighter-rouge">generate_bad_reads_summary.sh</code> that contains the appropriate SLURM directives. When finished, submit this job to the cluster.</p>

<h3 id="managing-jobs-and-getting-job-information">Managing jobs and getting job information</h3>

<p>There are several commands that you can use to control and get info about your jobs:</p>

<p><code class="highlighter-rouge">scancel</code> will become your friend! At some point, you’ll fire off one or more jobs, and realize you’ve
made a mistake. (What? You don’t make them? Then you can forget about this command) Here are a 
few examples of <code class="highlighter-rouge">scancel</code> in action:</p>

<p><code class="highlighter-rouge">bash
scancel JOBID										# specific job
scancel -u bfreeman								    # ALL my jobs
scancel -u bfreeman -J many_blast_jobs				# named jobs
scancel -u bfreeman -p bigmem						# ALL in partition
</code>
<code class="highlighter-rouge">squeue</code> will give you pending (to be done), running, and recently completed job info. Some examples:</p>

<p><code class="highlighter-rouge">bash
squeue -u bfreeman									# jobs for bfreeman
squeue -u bfreeman --states=R | wc –l				# # of Running jobs
</code></p>

<p><code class="highlighter-rouge">sacct</code> will give you current and historical information, since time began or you were an HPC-infant,
whichever came first. More examples:</p>

<p><code class="highlighter-rouge">bash
sacct -u bfreeman									# jobs for bfreeman
sacct -u bfreeman -p bigmem --starttime=9/1/14		# same+bigmem partition
sacct -j JOBID --format=JobID,JobName,ReqMem,MaxRSS,Elapsed # RAM requested &amp; used!!
</code></p>

<p><strong>Other info</strong>
See our list of common SLURM commands at https://rc.fas.harvard.edu/resources/documentation/convenient-slurm-commands/</p>

<p>For those coming from another cluster/scheduler, check our our scheduler Rosetta stone: http://slurm.schedmd.com/rosetta.pdf</p>

<p><strong>Exercises</strong>
At the start of these exercises, your instructor will start a number of jobs. Once started, please
complete the following questions:
* What <code class="highlighter-rouge">scancel</code> command would you issue to cancel pending jobs in the serial_requeue partition?
* What <code class="highlighter-rouge">squeue</code> command would you use to count how many jobs are in each state?
* What <code class="highlighter-rouge">sacct</code> command would you use to see all your jobs named BLAST run between Aug 1st and Aug 15th?
* (Bonus!) Give the command to ….</p>

<h2 id="best-practices">Best Practices</h2>

<p>Again, working on a cluster is working in a big sandbox, with people of all ages and skills. So it is
important to work carefully and be considerate. Please visit our list of Common Pitfalls and 
Fair Use/Responsibilities pages so that you’ll be a good member of the community…</p>

<p>Common Pitfalls: https://rc.fas.harvard.edu/resources/documentation/common-odyssey-pitfalls/<br />
Fair Use/Responsibilities: https://rc.fas.harvard.edu/resources/responsibilities/</p>

<h2 id="distributed-system-definitions-and-stacks">Distributed System Definitions and stacks:</h2>
<p>(Note that many definitions exist for these terms)</p>

<ul>
  <li>Distributed application: an application that can be executed on a distributed system platform (e.g., mpiBLAST)</li>
  <li>Distributed system platform: software layers that facilitates coordination and management of a distributed system (e.g., queue-based system, and MapReduce)</li>
  <li>Distributed system:
    <ul>
      <li>High Performance Computing (HPC): large assemble of physical machines and a homogeneous operating system (e.g., your institutions’ HPC, XSEDE’s HPC)</li>
      <li>Cloud Computing: virtual machines, distributed platforms and/or applications offered as a service (e.g., Amazon Web Services, Microsoft Azure, Google Cloud Computing)</li>
    </ul>
  </li>
  <li>Virtual machine (VM): software computer that like a physical computer can run an operating system and applications</li>
  <li>Operating system (OS): the basic software layer that allows execution and management of applications</li>
  <li>Physical machine: the hardware (processors, memory, disk and network)</li>
</ul>

<h2 id="hpc-vs-cloud">HPC vs. Cloud:</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: left">HPC</th>
      <th style="text-align: left">Cloud</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">User account on the system</td>
      <td style="text-align: left">root account on the system</td>
    </tr>
    <tr>
      <td style="text-align: left">Limited control of the system</td>
      <td style="text-align: left">Full control of the system</td>
    </tr>
    <tr>
      <td style="text-align: left">Central shared file system</td>
      <td style="text-align: left">Local file system</td>
    </tr>
    <tr>
      <td style="text-align: left">Jobs submitted into a queue</td>
      <td style="text-align: left">Jobs executed on each resource</td>
    </tr>
    <tr>
      <td style="text-align: left">Account-based isolation</td>
      <td style="text-align: left">OS-based isolation</td>
    </tr>
    <tr>
      <td style="text-align: left">Batch-oriented execution of applications</td>
      <td style="text-align: left">support for batch or interactive applications</td>
    </tr>
    <tr>
      <td style="text-align: left">Request for resource and time allocation</td>
      <td style="text-align: left">Pay-per-use</td>
    </tr>
    <tr>
      <td style="text-align: left">etc.</td>
      <td style="text-align: left">etc.</td>
    </tr>
  </tbody>
</table>

<p><img src="https://raw.githubusercontent.com/datacarpentry/cloud-genomics/master/lessons/images/HpcVsCloud.png" alt="HPC vs. Cloud" /></p>

<h2 id="resources">Resources:</h2>
<ul>
  <li>HPC offerings:</li>
  <li>Odyssey: https://rc.fas.harvard.edu/odyssey/</li>
  <li>XSEDE: https://www.xsede.org/high-performance-computing</li>
  <li>Cloud computing offerings:</li>
  <li>Amazon EC2: http://aws.amazon.com/ec2/</li>
  <li>Microsoft Azure: https://azure.microsoft.com/en-us/</li>
  <li>Google Cloud Platform: https://cloud.google.com/</li>
  <li>iPlant’s Atmosphere: http://www.iplantcollaborative.org/ci/atmosphere</li>
</ul>
